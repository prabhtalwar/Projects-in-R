---
title: "ML Assignment 1"
author: "Prabh Talwar"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(dplyr)
library(ggplot2)
library(rsample)
library(caret)
library(visdat)
```

```{r}
# Accessing data
ames <- AmesHousing::make_ames()

dim(ames)
```

```{r}
# Visualizing missing data
vis_miss(ames)

```

The above dataset has 2930 observations and 81 variables. The data set has both Numerical and Categorical data. While visualizing the data, we came to know that there is no missing values in the data set. 

# Splitting the dataset
```{r}
set.seed(123) # for reproducibility

split <- initial_split(ames, prop = 0.7, strata = "Sale_Price")
ames_train <- training(split)
ames_test <- testing(split)
```

To build a model that predicts well to our past data, we split our data in training and testing data sets. We are splitting our data into 70% training and 30% testing.We used stratified sampling here so that we can control the unbalanced distribution of the response variable as our response variable is positively skewed. Stratified sampling will ensure that our response variable, i.e; Sale Price is properly distributed in our training and testing data.

```{r}
ggplot(data = ames, mapping = aes(x = Sale_Price)) +
  geom_histogram()
```


```{r}
# Specify resampling strategy
cv <- trainControl(
method = "repeatedcv",
number = 10,
repeats = 5
)
```

We will be using K-fold cross validation that will randomly divides our training data set in to k folds. We are using 10 folds that will be repeated 5 times as or dat is not that big, i.e; it is $n<1000$.

```{r}
# Create grid of hyperparameter values
hyper_grid <- expand.grid(k = seq(2, 25, by = 1))
```

We are tuning parameters to control the complexity of the ML algorithms. we specified the hyperparameter values to 2 to 25.

```{r}
# Tune a knn model using grid search
knn_fit <- train(
Sale_Price ~ .,
data = ames_train,
method = "knn",
trControl = cv,
tuneGrid = hyper_grid,
metric = "RMSE"
)

knn_fit
```
we trained k-nearest neighbor model using "knn" method with pre-specified resampling procedure trControl, grid search, and loss function "RMSE".
